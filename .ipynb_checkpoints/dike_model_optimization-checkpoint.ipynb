{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ema_workbench/em_framework/evaluators.py:58: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n"
     ]
    }
   ],
   "source": [
    "from ema_workbench import (\n",
    "    Model,\n",
    "    MultiprocessingEvaluator,\n",
    "    ScalarOutcome,\n",
    "    IntegerParameter,\n",
    "    optimize,\n",
    "    Scenario,\n",
    ")\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress\n",
    "from ema_workbench.util import ema_logging\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine optimal policies\n",
    "### Step 1: Problem formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench.em_framework.optimization import (ArchiveLogger,\n",
    "                                                     EpsilonProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started with 8 workers\n",
      "  0%|                                                | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "model, steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "reference_values = {\n",
    "        \"Bmax\": 175,\n",
    "        \"Brate\": 1.5,\n",
    "        \"pfail\": 0.5,\n",
    "        \"discount rate 0\": 3.5,\n",
    "        \"discount rate 1\": 3.5,\n",
    "        \"discount rate 2\": 3.5,\n",
    "        \"discount rate 3\": 3.5,\n",
    "        \"ID flood wave shape\": 4,\n",
    "    }\n",
    "scen1 = {}\n",
    "\n",
    "for key in model.uncertainties:\n",
    "    name_split = key.name.split(\"_\")\n",
    "\n",
    "    if len(name_split) == 1:\n",
    "        scen1.update({key.name: reference_values[key.name]})\n",
    "\n",
    "    else:\n",
    "        scen1.update({key.name: reference_values[name_split[1]]})\n",
    "\n",
    "ref_scenario = Scenario(\"reference\", **scen1)\n",
    "\n",
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./data\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"optimization.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "epsilon = [1e3] * len(model.outcomes)\n",
    "\n",
    "nfe = 10000  # proof of principle only, way to low for actual use\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(\n",
    "            nfe=nfe,\n",
    "            searchover=\"levers\",\n",
    "            epsilons=epsilon,\n",
    "            convergence=convergence_metrics,\n",
    "            reference=ref_scenario,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "archives = ArchiveLogger.load_archives(\"./data/optimization.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench.em_framework.optimization import to_problem\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "# hv = HypervolumeMetric(reference_set, problem)\n",
    "# hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "# hypervolume.sort(key=lambda x:x[0])\n",
    "# hypervolume = np.asarray(hypervolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "# ax3.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "# ax3.set_ylabel('hypervolume')\n",
    "\n",
    "ax2.plot(convergence.epsilon_progress)\n",
    "ax2.set_xlabel(\"nr. of generations\")\n",
    "ax2.set_ylabel(r\"$\\epsilon$ progress\")\n",
    "sns.despine()\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "# ax3.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Searching for candidate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "results.iloc[:,21:41].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "results['sum dike 1 & 2'] = results.loc[:, '0_RfR 0':'1_RfR 3'].sum(axis=1)\n",
    "results['height dike 1'] = results.loc[:, 'A.1_DikeIncrease 0':'A.1_DikeIncrease 3'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "selection = results[results['sum dike 1 & 2'] == 0]\n",
    "final_selection = selection[selection['height dike 1'] >= 5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "policies = final_selection.loc[:, '0_RfR 0':'A.5_DikeIncrease 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f'We continue with {len(policies)} policies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Re-evaluate candidate solutions under uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "\n",
    "    logical = experiments['policy']==policy\n",
    "\n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def calculate_regret(data, best):\n",
    "    return np.abs(best-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in model.outcomes:\n",
    "    policy_column = experiments['policy']\n",
    "\n",
    "    # create a DataFrame with all the relevent information\n",
    "    # i.e., policy, scenario_id, and scores\n",
    "    data = pd.DataFrame({outcome.name: outcomes[outcome.name],\n",
    "                         \"policy\":experiments['policy'],\n",
    "                         \"scenario\":experiments['scenario']})\n",
    "\n",
    "    # reorient the data by indexing with policy and scenario id\n",
    "    data = data.pivot(index='scenario', columns='policy')\n",
    "\n",
    "    # flatten the resulting hierarchical index resulting from\n",
    "    # pivoting, (might be a nicer solution possible)\n",
    "    data.columns = data.columns.get_level_values(1)\n",
    "\n",
    "    # we need to control the broadcasting.\n",
    "    # max returns a 1d vector across scenario id. By passing\n",
    "    # np.newaxis we ensure that the shape is the same as the data\n",
    "    # next we take the absolute value\n",
    "    #\n",
    "    # basically we take the difference of the maximum across\n",
    "    # the row and the actual values in the row\n",
    "    #\n",
    "    outcome_regret = (data.max(axis=1).values[:, np.newaxis] - data).abs()\n",
    "\n",
    "    overall_regret[outcome.name] = outcome_regret\n",
    "    max_regret[outcome.name] = outcome_regret.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "max_regret = pd.DataFrame(max_regret)\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "x = experiments.loc[:, :'discount rate 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "outcomes['Expected Annual Damage'].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "np.percentile(outcomes['Expected Annual Damage'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import prim\n",
    "\n",
    "x = experiments\n",
    "y = outcomes['Expected Annual Damage'] < 4e7\n",
    "\n",
    "prim_alg = prim.Prim(x, y, threshold=0.5, peel_alpha=0.005)\n",
    "box = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "box.show_tradeoff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "box.inspect(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scenario MORDM\n",
    "### Select scenarios based on maximum diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "experiments_of_interest = experiments.loc[y]\n",
    "outcomes_df = pd.DataFrame({k:v[y] for k,v in outcomes.items()})\n",
    "\n",
    "# normalize outcomes on unit interval to ensure equal weighting of outcomes\n",
    "x = outcomes_df.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "normalized_outcomes = pd.DataFrame(x_scaled, columns=outcomes_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "n_scen = experiments.loc[y].shape[0]\n",
    "indices = range(n_scen)\n",
    "set_size = 4\n",
    "n_scen\n",
    "combinations = itertools.combinations(indices, set_size)\n",
    "combinations = list(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# sampled_combinations = random.sample(combinations, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
